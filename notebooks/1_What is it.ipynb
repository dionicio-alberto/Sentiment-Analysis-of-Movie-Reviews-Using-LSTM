{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Movie Reviews Using LSTM\n",
    "\n",
    "We will turn our attention to recurrent neural networks (RNNs), in particular, to long short-term memory (LSTM) networks and how they can be used in sequencial problems, such as Natural Lenguage Processing (NPL). We will develop and train a LSTM network to predict the sentiment of movie reviews on IMDb\n",
    "\n",
    "\n",
    "-----\n",
    "## Sequential problems in machine learning\n",
    "\n",
    "Sequential problems are a class of problems in machine learning in which the order of the features presented to the model is important for making predictions. These are commonly encountered in the following scenarios:\n",
    "\n",
    "- NPL: sentiment analysis, lenguage translation, text prediction\n",
    "- Time series predictions\n",
    "\n",
    "Many NLP problems are sequential problems, because the languages that we speak are sequential in nature, and the sequence conveys context and other subtle nuances.\n",
    "\n",
    "Sequential problems also occur naturally in time series proble,s. Time series problems are common in stock markets.\n",
    "\n",
    "-----\n",
    "## NLP and sentiment analsys\n",
    "\n",
    "NLP is a subfiled in artificial intelligence that is concerned with the interaction of computers and human lenguages. \n",
    "\n",
    "With the proliferation of deep learning and neural networks in the image classification domian, scientists began to wonder whether the powers of neural networks could be applied to NPL. The ability of AI assistants, such as Siri and Alexa, to understand multiple languages spoken in different accents was the result of deep learning and LSTM networks.\n",
    "\n",
    "Sentiment analysis is also an area of NLP that benefited from the resurgence of deep learning. It is defined as the prediction of the positivity of a text. Most sentiment analysis problems are classification problems (psotive, neutral or negative) and not regression problems. \n",
    "\n",
    "-----\n",
    "## Why sentiment analysis is difficult\n",
    "\n",
    "Due to the presence of subtle nuances in human lenguage. The same word can often covey a differnet meaning depending on the context.\n",
    "\n",
    "Another reason sentiment analysis is difficult is because of sarcasm\n",
    "\n",
    "-----\n",
    "## RNN (Recurrent nerual networks)\n",
    "\n",
    "An RNN has high-level architecture, as shown in the following diagram\n",
    "\n",
    "![diagram](https://i.imgur.com/mIUxURI.png)\n",
    "\n",
    "We can see that an RNN is a multi-layered neural networl. We can break up the raw input, splitting it into time steps. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## What's inside an RNN?\n",
    "\n",
    "The following diagram depicts the mathematical function inside each layer of an RNN:\n",
    "\n",
    "![diagram2](https://i.imgur.com/SjCsxjw.png)\n",
    "\n",
    "The mathematical function of an RNN is simple. Each layer $t$ within an RNN has two inputs\n",
    "\n",
    "- The input from the time step $t$\n",
    "- The gidden state passed from the previus layer $t-1$\n",
    "\n",
    "Each layer in RNN simply sums up the two inputs and applies a tanh($x$) function to the sum. It then outputs the result, to be passed as a hidden state to the next layer. More formally, the output hidden state of layer $t$ is this \n",
    "\n",
    "$$ s_t = \\tanh (s_{t-1}+x_t) $$\n",
    "\n",
    "The tanh function is a good choice as a non-linear transformation of the combinations of the current input and the previus hidden state, becaus it ensure that the weights don't diverge too rapidly and is easily differentiable.\n",
    "\n",
    "Finally, to get the final output from the last layer in the RNN we apply a sigmoid function.\n",
    "\n",
    "We can see that if we stack these layers together, the final output from an RNN depends on the non-linear combination of the inputs at different time steps.\n",
    "\n",
    "-----\n",
    "\n",
    "## Long- and short-term dependecies in RNN's\n",
    "\n",
    "The architecture of an RNN makes it ideal for handling sequential data. \n",
    "\n",
    "Let's first take a look at a short piece of text as our sequential data:\n",
    "\n",
    "> **THE WEATHER IS HOT TODAY**\n",
    "\n",
    "We can treat this short sentence as sequential data by breaking it down into five different inputs. \n",
    "\n",
    "![diagram](https://i.imgur.com/5uzfiXo.png)\n",
    "\n",
    "Suposse that we are building a simple RNN to predict whether is it snowing based on this sequential data. The RNN would look something like this:\n",
    "\n",
    "![example](https://i.imgur.com/I1w5m9o.png)\n",
    "\n",
    "The critical piece of information in the sequence is the word hot, at time step 4. Notice that the critical piece of information came just shortly before the final input. We would say that there is a short-term dependency in this sequence. \n",
    "\n",
    "Let's take a look at longer sequence of text.\n",
    "\n",
    "> \"I really liked the movie but i was disappointed in the service and cleanliness of the cinema. The cinema should be better maintained in order to provide a better experience for customers.\"\n",
    "\n",
    "Our goal is to predict whether the customer liked the movie. The customer liked the movie but not the cinema, which was the main complaint in the paragraph. The RNN would look this:\n",
    "\n",
    "![sequence](https://i.imgur.com/5lBgq42.png)\n",
    "\n",
    "The critical words **liked the movie** appared between time steps 3 and 5. Notice that there is a significant gap between the critical time steps and the output time step, as the rest of the text was largely irrelevant to the prediction problem. We say that there is a long-term dependency in this sequence. Unfortunately, RNNs do not work well with long-term dependency sequences. RNNs have a good a good short-term memory but a bad long-term memory. \n",
    "\n",
    "-----\n",
    "\n",
    "## The vanishing gradient problem\n",
    "\n",
    "It is a problem when training deep neural networks using gradient-based methodss such as backpropagation.\n",
    "\n",
    "When the loss is propagated backward, the loss tends to decrase with each successive layer:\n",
    "\n",
    "![loss](https://i.imgur.com/1NaXFtc.png)\n",
    "\n",
    "As a result, by the time the loss is propagated back toward the first few layers, the loss has already diminished so much that the weights do not change much at all. With such a small loss being propagated backward, it is impossible to adjust and train the weights of the first few layers. This phenomenon is known as the *vanishing grading problem in machine larning* \n",
    "\n",
    "To address this problem, Hochreiter and other proposed a clever varation of the RNN, known as the long short-term memory (LSTM) network.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# The LSTM network\n",
    "\n",
    "LSTM are a variation of RNNs, and they solve the long-term dependency problem faced by conventional RNNs. \n",
    "\n",
    "-----\n",
    "\n",
    "## What's inside an LSTM network?\n",
    "\n",
    "LSTMs have the same repeating structure of RNNs that we have seen previously. However, LSTM's diffier in their internal structure.\n",
    "\n",
    "The diagram shows a high-level overview of the repeating unit of an LSTM\n",
    "\n",
    "![unit](https://i.imgur.com/gSfie8Q.png)\n",
    "\n",
    "An LSTM differs from a conventional RNN in that it has a cell state, in addition to the hidden state. You can think of the cell state as the current memory of the LSTM. In contrast, the hidden state is the overall memory of the entire LSTM. It contains everything that we have seen so far, both important and unimportant information. \n",
    "\n",
    "How does the LSTM release information between the hidden state and the cell state? It does so via three important gates:\n",
    "\n",
    "- Forget gate\n",
    "- Input gate\n",
    "- Output gate\n",
    "\n",
    "Just like physical gates, the three gates restrict the flow of information from the hidden state to the cell state. \n",
    "\n",
    "-----\n",
    "\n",
    "## Forget gate \n",
    "\n",
    "![forget gate](https://i.imgur.com/Bd11o5b.png)\n",
    "\n",
    "The **Forget gate (f)** forms the first part of the LSTM repeating unit, and its role is to decide how much data we should forget or remember from the previous cell state. It does so by first concatenating the Previous Hidden State $(h_{tâˆ’1})$ and the current Input $(x_t)$, then passing the concatenated vector through a sigmoid function.\n",
    "\n",
    "-----\n",
    "\n",
    "# Input gate\n",
    "\n",
    "The next gate is the **Input gate (i)**, it controls how much information to pass to the current cell state.\n",
    "\n",
    "![input gate](https://i.imgur.com/95MmUo5.png)\n",
    "\n",
    "Just like the forget gate, the **Input gate(i)** takes as input the concatenation of the **Previus Hidden State** and the current **Input**. It then passes two copies of the concatenated vector through a sigmoid function and a tanh function, before multiplying them together.\n",
    "\n",
    "The output of the input gate, $i$, is a follows:\n",
    "\n",
    "$$ i = \\sigma( \\textrm{concatenate}(h_{t_1},x_t)) \\ast \\tanh ( \\textrm{concatenate}(h_{t_1},x_t)) $$ \n",
    "\n",
    "At this point, we have what is required to compute the current cell state ($C_t$) to be output.\n",
    "\n",
    "![current state](https://i.imgur.com/jJk48Cz.png)\n",
    "\n",
    "The current cell state $C_t$ is as follows\n",
    "\n",
    "$$ C_t = ( f \\ast C_{t-1}) + i $$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Output gate\n",
    "\n",
    "Output gate controls how much information is to be retained in the hidden state. \n",
    "\n",
    "![output gate](https://i.imgur.com/q8tbuDt.png)\n",
    "\n",
    "First, we concatenate the **Previus Hidden State (h_{t-1})** and the current **Input(x_t)**, and pass it through a sigmoid function. Then. we take the current cell state ($C_t$) and pass it through a tanh function. Finally, we take the multiplication of the two, which is passed to the next repeating unit as the hidden state ($h_t$). This process is summarized by the following equation:\n",
    "\n",
    "$$h_t = \\sigma( \\textrm{concatenate}(h_{t-1},x_t)) \\ast \\tanh(C_t) $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural-network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
